{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intend to experiment with sentiment analysis (sentence classification) using different word\n",
    "embeddings and the Sentiment140 dataset containing labeled Twitter tweets. More specifically,\n",
    "we intend to train our own word embeddings using word2vec and then use those with a deep\n",
    "learning sentiment analysis model (TBD: LSTM/GRU/Transformer) that we will create using\n",
    "Pytorch. The main experiments we will perform are\n",
    "- Use Pytorch’s trainable embeddings with random initialization\n",
    "- Use Pytorch’s trainable embeddings with our trained word2vec initialization\n",
    "- Only use our trained word2vec embeddings as inputs\n",
    "\n",
    "and then compare the accuracy results for the sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would suggest training several sets of your own embeddings (experiment with the parameters to see how they influence the final vectors). Then, compare the sets of embeddings outside of your system (analogies,odd-one-out...), so you can set some expectations about what embeddings might yield the best result for your task. Finally, look at how the vectors perform in your system and analyze if you expected such result and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
    "\n",
    "- https://github.com/hietalajulius/deep-learning-aalto/blob/master/Classifier.ipynb\n",
    "\n",
    "- https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.vocab\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>oop wrong url thing work mind brain foggi life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>yes fantast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>pretttyyi pleaseee tweet mileycsupport reallll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>yep heard everi sad song twitter safe say far ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>oh got blush like littl girl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1     oop wrong url thing work mind brain foggi life\n",
       "1       1                                        yes fantast\n",
       "2       1  pretttyyi pleaseee tweet mileycsupport reallll...\n",
       "3       0  yep heard everi sad song twitter safe say far ...\n",
       "4       1                       oh got blush like littl girl"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/processed_train.csv')\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embeddings\n",
    "- word2Vec\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nglove = torchtext.vocab.Glove(name='6B', dim=100)\\n\\nclosest(glove, get_vector(glove, 'paper'))\\n\\nanalogy(glove, 'moon', 'night', 'sun')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vector(embeddings, word):\n",
    "    \n",
    "    return embeddings.vectors(embeddings.stoi[word])\n",
    "\n",
    "\n",
    "def closest(embeddings, vector, n=6):\n",
    "    disntances = []\n",
    "    for neighbor in embeddings.itos:\n",
    "        distances.append(neighbor, torch.dist(vector, get_vactor(embeddings, neighbor)))\n",
    "    \n",
    "    return sorted(distances, key=lambda x: x[1])[:n]\n",
    "\n",
    "\n",
    "def analogy(embeddings, w1, w2, w3, n=6):\n",
    "    \n",
    "    closest_words = closest(embeddings,\n",
    "                           get_vector(embeddings, w2) \\\n",
    "                            - get_vector(embeddings, w1) \\\n",
    "                            + get_vector(embeddings, w3),\n",
    "                           n + 3)\n",
    "    closest_words = [x for x in closest_words if x[0] not in [w1, w2, w3]][:n]\n",
    "    \n",
    "    return closest_words\n",
    "\n",
    "\"\"\"\n",
    "glove = torchtext.vocab.Glove(name='6B', dim=100)\n",
    "\n",
    "closest(glove, get_vector(glove, 'paper'))\n",
    "\n",
    "analogy(glove, 'moon', 'night', 'sun')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TEXT = torchtext.data.Field(tokenize= 'spacy',\n",
    "                            init_token='< sos >',\n",
    "                            eos_token='< eos >',\n",
    "                            unk_token='< unk >',\n",
    "                            tokenizer_language='en_core_web_sm',\n",
    "                            lower=True)\n",
    "\"\"\"\n",
    "TEXT = torchtext.data.Field(tokenize= 'spacy',\n",
    "                            tokenizer_language='en_core_web_sm',\n",
    "                            lower=True)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.float)\n",
    "\n",
    "datafields = [('Sentiment', LABEL), ('SentimentText', TEXT)]\n",
    "\n",
    "train, val, test = torchtext.data.TabularDataset.splits(path='data/',\n",
    "                                                  train='processed_train.csv',\n",
    "                                                  validation='processed_val.csv',\n",
    "                                                  test='processed_test.csv',\n",
    "                                                  format='csv',\n",
    "                                                  skip_header=True,\n",
    "                                                  fields=datafields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "[('go', 88779), ('get', 70792), ('day', 67839), ('good', 59384), ('work', 56051), ('like', 53493), ('love', 52734), ('quot', 47034), ('got', 45559), ('today', 43640), ('time', 42450), ('nt', 39689), ('lol', 38152), ('thank', 38138), ('back', 36807), ('want', 36771), ('one', 36690), ('i', 36474), ('miss', 36311), ('u', 35543)]\n",
      "['<unk>', '<pad>', 'go', 'get', 'day', 'good', 'work', 'like', 'love', 'quot']\n",
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "print(TEXT.vocab.itos[:10])\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# minimise badding for each sentence\n",
    "train_iterator, val_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "                                (train, val, test),\n",
    "                                batch_size=64,\n",
    "                                sort_key=lambda x: len(x.SentimentText),\n",
    "                                sort_within_batch=False,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_dim,\n",
    "                         num_layers=n_layers,\n",
    "                         bidirectional=bidirectional,\n",
    "                         dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded_text = self.dropout(self.embedding(text))\n",
    "        output, hidden = self.gru(embedded_text)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        x = self.fc(hidden.squeeze(0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (embedding): Embedding(25002, 100)\n",
      "  (gru): GRU(100, 256, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = GRU(vocab_size=INPUT_DIM, \n",
    "            embedding_dim=EMBEDDING_DIM, \n",
    "            hidden_dim=HIDDEN_DIM, \n",
    "            output_dim=OUTPUT_DIM, \n",
    "            n_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=0.1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#pretrained_embeddings = TEXT.vocab.vectors\\n#model.embedding.weight.data.copy_(pretrained_embeddings)\\n\\nunk_idx = TEXT.vocab.stoi[TEXT.unk_token]\\npad_idx = TEXT.vocab.stoi[TEXT.pad_token]\\n\\nmodel.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\\nmodel.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pretrained embeddings\n",
    "\"\"\"\n",
    "#pretrained_embeddings = TEXT.vocab.vectors\n",
    "#model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.Sentiment)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return model, epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            # print(batch.SentimentText)\n",
    "            if batch.SentimentText.nelement() > 0:\n",
    "                predictions = model(batch.SentimentText).squeeze(1)\n",
    "\n",
    "                loss = criterion(predictions, batch.Sentiment)\n",
    "\n",
    "                acc = binary_accuracy(predictions, batch.Sentiment)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "            # else:\n",
    "              #  print(f\"Found a non-empty Tensorlist {batch.SentimentText}\")\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 18m 9s\n",
      "\tTrain Loss: 0.667 | Train Acc: 59.14%\n",
      "\t Val. Loss: 0.656 |  Val. Acc: 59.88%\n",
      "Epoch: 02 | Epoch Time: 18m 16s\n",
      "\tTrain Loss: 0.657 | Train Acc: 60.61%\n",
      "\t Val. Loss: 0.649 |  Val. Acc: 61.08%\n",
      "Epoch: 03 | Epoch Time: 18m 16s\n",
      "\tTrain Loss: 0.649 | Train Acc: 61.75%\n",
      "\t Val. Loss: 0.641 |  Val. Acc: 62.29%\n",
      "Epoch: 04 | Epoch Time: 18m 16s\n",
      "\tTrain Loss: 0.638 | Train Acc: 63.20%\n",
      "\t Val. Loss: 0.627 |  Val. Acc: 64.07%\n",
      "Epoch: 05 | Epoch Time: 18m 22s\n",
      "\tTrain Loss: 0.620 | Train Acc: 65.24%\n",
      "\t Val. Loss: 0.606 |  Val. Acc: 66.33%\n",
      "Epoch: 06 | Epoch Time: 18m 17s\n",
      "\tTrain Loss: 0.596 | Train Acc: 67.56%\n",
      "\t Val. Loss: 0.580 |  Val. Acc: 69.06%\n",
      "Epoch: 07 | Epoch Time: 18m 35s\n",
      "\tTrain Loss: 0.577 | Train Acc: 69.24%\n",
      "\t Val. Loss: 0.566 |  Val. Acc: 70.27%\n",
      "Epoch: 08 | Epoch Time: 18m 16s\n",
      "\tTrain Loss: 0.565 | Train Acc: 70.34%\n",
      "\t Val. Loss: 0.557 |  Val. Acc: 71.17%\n",
      "Epoch: 09 | Epoch Time: 18m 18s\n",
      "\tTrain Loss: 0.556 | Train Acc: 71.11%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 71.92%\n",
      "Epoch: 10 | Epoch Time: 18m 37s\n",
      "\tTrain Loss: 0.550 | Train Acc: 71.72%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 72.21%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model, train_loss, train_acc  = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, val_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'gru_simple_.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (embedding): Embedding(25002, 100)\n",
      "  (gru): GRU(100, 256, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Test Loss: 0.547 | Test Acc: 72.14%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('gru_simple_.pt'))\n",
    "print(model)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
